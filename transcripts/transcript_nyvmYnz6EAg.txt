I’ve had a lot of discussions on my podcast 
where we haggle out our timelines to AGI.  Some guests think it’s 20 
years away. Others 2 years.  Here’s where my thoughts lie as of July 2025.
Sometimes people say that even if all AI   progress totally stopped, the systems of 
today would still be far more economically   transformative than the internet. I disagree. 
I think the LLMs of today are magical.  But the reason that the Fortune 500 aren’t using 
them to totally transform their workflows isn’t   because the management there is too stodgy.
Rather, I think it’s genuinely hard to get   normal humanlike labor out of these LLMs.
And this has to do with some fundamental   capabilities these models lack.
I like to think I’m "AI forward"   here at the Dwarkesh Podcast.
I’ve probably spent over a hundred   hours trying to build these little LLM 
tools for my post production setup.  And the experience of trying to get them 
to be useful has extended my timelines.  I’ll try to get an LLMs to rewrite autogenerated 
transcripts for me, to optimize for readability   in the way a human would.
Or I’ll try to get them to   identify clips from a transcript that I feed in.
Sometimes I’ll try to get them to co-write an   essay with me, passage by passage.
These are simple, self contained,   short horizon, language in, language out tasks—the 
kinds of assignments that should be dead center   in the LLMs’ repertoire. And they're 5/10 at 
them. Don’t get me wrong, that is impressive.  But the fundamental problem is that LLMs don’t 
get better over time the way a human would.  This lack of continual learning 
is a huge huge bottleneck.  The LLM baseline at many tasks might 
be higher than the average human's.  But there’s no way to give 
a model high level feedback.  You’re stuck with the abilities 
you get out of the box.  You can keep messing around with the system 
prompt, but in practice this just doesn’t produce   anything even close to the kind of learning and 
improvement that human employees experience.  The reason humans are so useful 
is not mainly their raw intellect.  It’s their ability to build up context, 
interrogate their own failures,   and pick up small improvements and 
efficiencies as they practice a task.  How would you teach a kid to play the saxophone?
You'd have them try to blow into one, and then   they'd see how it sounds, and they'd adjust.
Now imagine if this was the way you'd have to   teach saxophone instead: A 
student takes one attempt.  And the moment they make a mistake, you 
send them away and you write detailed   instructions about what went wrong.
And you call the next student in.  And the next student reads your notes 
and tries to play Charlie Parker cold.  And when they fail, you refine your instructions 
and you invite the next student. This just   wouldn’t work. No matter how well honed your 
prompt is, no kid is just going to learn how   to play saxophone from reading your instructions.
But this is the only modality we have to ‘teach’   LLMs anything. Yes, there’s RL fine tuning. 
But it’s not a deliberate, adaptive process   in the way that human learning is.
My editors have gotten extremely good.  And they wouldn’t have gotten that way if 
we had to build bespoke RL environments for   different subtasks involved in their work.
They’ve just noticed a lot of small things   themselves and thought hard about 
what resonates with the audience,   what kind of content I like, and how they 
can improve their day to day workflows.  Now, it’s possible to imagine ways in 
which a smarter model could build a   dedicated RL loop for itself which just 
feels super organic from the outside.  I give some high level feedback, and the 
model comes up with a bunch of verifiable   practice problems to RL on—maybe even a whole 
environment in which it gets to rehearse the   skills that it thinks it's lacking.
But this just sounds really hard.  And I don’t know how well these 
techniques will generalize to   different kinds of tasks and feedback.
Eventually the models will be able to learn   on the job in this organic way that humans can.
But it’s just hard for me to see how that could   happen within the next few years, given 
there’s no immediately obvious way in   which to slot in continuous learning into 
the kinds of models that these LLMs are.  LLMs actually do get kinda smart and 
useful in the middle of a session.  For example, sometimes I’ll 
co-write an essay with an LLM.  I’ll give it an outline, and I’ll ask it 
to draft the essay passage by passage.  And all its suggestions up till 
paragraph four will just be bad.  I'll just rewrite every single paragraph from 
scratch and tell it, "Look, your shit sucked.  This is what I wrote instead."
And at this point, it will actually start   giving good suggestions for the next paragraph.
But this whole subtle understanding of my   preferences and style will just 
be lost by the end of the session.  Maybe there is an easy solution to this that 
looks like a long rolling context window, like   Claude Code already has, which just compacts the 
session memory into a summary every 30 minutes.  I just think that titrating all this rich 
tacit experience into a text summary will   be brittle in domains outside of software 
engineering, which is very text-based,   in which you already have this external scaffold 
of memory that is stored in the codebase itself.  Again, think about what it would be like to 
teach a kid to play the saxophone just from text.  Even Claude Code will often reverse a hard-earned 
optimization that we engineered together before I   hit /compact —because the explanation for why 
it was made didn’t make it into the summary.  This is why I disagree with something that 
Anthropic researchers Sholto Douglas and   Trenton Bricken said on my podcast.
And this quote is from Trenton:  "Even if AI progress totally stalls 
(and you think that the models are   really spiky, and they don't have general 
intelligence), it's so economically valuable,   and sufficiently easy to collect data on all 
of these different white collar job tasks,   such that to Sholto's point we should expect to 
see them automated within the next five years."  If AI progress totally stops today, I think less 
than 25% of white collar employment goes away.  Sure, many tasks will get automated.
Claude 4 Opus can technically rewrite   autogenerated transcripts for me.
But since it’s not possible for me   to have it improve over time and learn my 
preferences, I still hire a human for this.  So even if we get more data, without progress 
in continual learning, I think that we will be   in a substantially similar position with 
all other kinds of white collar work.  Yes, technically AIs will be able to do 
a lot of subtasks somewhat satisfactorily,   but their inability to build up context 
will make it impossible to have them operate   as actual employees at your firm.
While this makes me bearish about   transformative AI in the next few years, it makes 
me especially bullish on AI over the next decades.  When we do solve continual learning, we’ll see a 
huge discontinuity in the value of these models.  Even if there isn’t a software only 
singularity, where these models rapidly   build smarter and smarter successor systems, 
we might still get something that looks like   a broadly deployed intelligence explosion.
AIs will be getting broadly deployed through the   economy, and doing different jobs and learning 
while doing them in the way that humans can.  However, unlike humans, these 
models can amalgamate their   learnings across all their copies.
So one AI is basically learning   how to do every single job in the economy.
An AI that is capable of this kind of online   learning might rapidly become a superintelligence 
even if there's no further algorithmic progress.  However, I’m not expecting to watch some 
OpenAI livestream where they announce that   continual learning has been totally solved.
Because labs are incentivized to release any   innovations quickly, we’ll see a broken early 
version of continual learning (or test time   training, or whatever you want to call it) before 
we see something which truly learns like a human.  I expect to get lots of heads up before 
this big bottleneck is totally solved.  When I interviewed Anthropic researchers 
Sholto Douglas and Trenton Bricken on my   podcast, they said that they expect reliable 
computer use agents by the end of next year.  We already have computer use agents right 
now, but they’re pretty bad. They’re imagining   something quite different. Their forecast is 
that by the end of next year, you should be   able to tell an AI, "Go do my taxes."
And it'ill go through all your email,   your Amazon orders, and Slack messages, and 
it will email back and forth with every single   person you need to get invoices from, it'll 
compile all your receipts, decide what things   are actually are business expenses, and it will 
ask for your approval on all the edge cases,   and then will just submit Form 1040 to the IRS. 
I’m skeptical. I’m not an AI researcher, so far be   it for me to contradict them on technical details.
But given what little I know, here’s why I’d bet   against this forecast:
One.  As horizon lengths increase, 
rollouts have to become longer.  The AI needs to do two hours worth of 
agentic computer use tasks before we   can even see if it did it right.
Not to mention that computer use   requires processing images and video, 
which is already more compute intensive,   even if you don’t factor in the longer rollouts.
This seems like it should slow down progress.   Two. We don’t have a large pretraining 
corpus of multimodal computer use data.  I like this quote from Mechanize’s post 
on automating software engineering:   "For the past decade of scaling, we’ve been 
spoiled by the enormous amount of internet   data that was freely available for us to use.
This was enough to crack natural language   processing, but not for getting models 
to become reliable, competent agents.  Imagine trying to train GPT-4 on all 
the text data available in 1980—the   data would have been nowhere near enough, 
even if you had the necessary compute."  Again, I’m not at the labs.
Maybe text only training already gives you a   great prior over how different UIs work, and what 
the relationship is between different components.  Maybe RL fine tuning is so sample efficient 
that you don’t need that much data.  But I haven’t seen any public evidence which 
makes me think that these models have suddenly   gotten less data hungry, especially in domains 
where they’re substantially less practiced.  Alternatively, maybe these models are such 
good front end coders that they can just   generate millions of toy UIs for themselves 
to practice on. But, three. Even algorithmic   innovations which seem quite simple in 
retrospect took a long time to iron out.  The RL procedure which DeepSeek explained in 
their R1 paper seems simple at a high level.  And yet it took 2 years from the development 
and launch of GPT-4 to the release of o1.  Now of course I know that it's insanely and 
hilariously arrogant to say that R1/o1 were easy—a   ton of engineering, debugging, and pruning of 
alternative ideas was required to arrive at   this solution. But that’s precisely my point! 
Seeing how long it took to implement the idea   of ‘We should train a model to solve verifiable 
math and coding problems,’ makes me think that   we’re underestimating the difficulty of solving 
the much gnarlier problem of computer use, where   you’re operating in a totally different modality 
with much less data. Okay, enough cold water. I’m   not going to be like one of those spoiled children 
on Hackernews who could be handed a golden-egg   laying goose and would still spend all their 
time complaining about how loud its quacks are.  Have you read the reasoning traces from o3 or 
Gemini 2.5? It’s actually reasoning! It’s breaking   down a problem, thinking through what the user 
wants, reacting to its own internal monologue,   and correcting itself when it notices that 
it's pursuing an unproductive direction.  How are we just like, "Oh yeah of course a 
machine is gonna go think a bunch, come up   with a bunch of ideas, and come back to me with 
a smart answer. That’s just what machines do."  Part of the reason some people are too 
pessimistic is that they haven’t played   around with the smartest models in 
domains where they’re the most competent.  Giving Claude Code a vague spec and just 
sitting around for 10 minutes while it   zero shots a working application is a wild 
experience. How did it do that? You can talk   about circuits and the training distribution 
and RL or whatever, but the most proximal,   concise, and accurate explanation is simply that 
it’s powered by a baby general intelligence.  At this point, part of you has to 
be thinking, "It’s actually working.  We’re making machines that are intelligent."
My probability distributions are super wide.  And I want to emphasize that I do 
believe in probability distributions.  Which means that work to prepare for a 
misaligned 2028 ASI still makes a ton of sense.  I think this is a totally plausible outcome.
But here are the timelines at which   I’d take a 50/50 bet.
An AI that can do taxes   end-to-end for my small business as well as 
a competent general manager could in a week:   including chasing down all the receipts on 
different websites, finding the missing pieces,   emailing back and forth with anyone who we need 
to hassle for invoices, filling out the form,   and sending it to the IRS. This I'd say 2028. I 
think we’re in the GPT 2 era for computer use.  But we have no pretraining corpus, and the 
models are optimizing for a much sparser   reward over a much longer time horizon using 
action primitives they’re unfamiliar with.  That being said, the base model is 
already decently smart and might have   a good prior over computer use tasks, 
plus there’s a lot more compute and AI   researchers in the world, so it might even out.
Preparing taxes for a small business feels like   for computer use what GPT 4 was for language.
It took 4 years to get from GPT 2 to GPT 4.  Just to clarify, I am not saying that 
we won’t have really cool computer use   demos in 2026 and 2027.
GPT-3 was super cool,   but it was not that practically useful.
I’m saying that these models won’t be capable   of end-to-end handling a week long and quite 
involved project which involves computer use.  Ok, and as for the forecast of when AI 
will be able to learn on the job as easily,   organically, seamlessly, and quickly 
as humans, for any white collar work.  For example, if I hired an AI video editor, after 
six months it would have as much actionable,   deep understanding of my preferences, 
our channel, what works for the audience,   as well as a human would.
I'd say this would come in 2032.  While I don’t see an obvious way to 
slot in continuous online learning   into the kinds of models these LLMs 
are, 7 years is a really long time!  GPT 1 had just come out this time 7 years ago.
It doesn’t seem implausible to me that over the   next 7 years, we’ll find some way to get 
these models to actually learn on the job.  At this point you might be reacting, 
"Wait you made this huge fuss about   continual learning being such a huge handicap.
But then your prediction is that we’re 7 years   away from what, at a minimum, looks like a broadly 
deployed intelligence explosion." And yeah, you’re   right. I am forecasting a pretty wild world within 
a relatively short amount of time. AGI timelines   are very lognormal. It's either this decade or 
bust. (Not really, it's more like lower marginal   probability per year—but that’s less catchy).
AI progress over the last decade has   been driven by scaling training 
compute on the frontier systems.  It's been over 4x a year.
This cannot continue beyond this decade,   whether you look at chips, power, or even the 
raw fraction of GDP that is used on training.  After 2030, AI progress has to mostly 
come from algorithmic progress.  But even there all the low 
hanging fruit will be plucked,   at least under the deep learning paradigm.
So the yearly probability of AGI collapses.  This means that if we end up on the longer side of 
my 50/50 bets, we might be looking at a relatively   normal world up till the 2030s or even the 2040s.
But in all the other worlds, even if we stay sober   about the current limitations of AI, we 
have to expect some truly crazy outcomes.  This was originally a blog post that I 
published on my website at dwarkesh.com.  And it was obviously inspired by the discussion 
I had with Sholto and Trenton on my podcast,   where I ended up disagreeing with them about 
timelines but it took me a few weeks of thinking   afterwards, sorting out exactly where I 
disagree and why I had longer timelines.  And I do this for other episodes as well.
I wrote up some thoughts I had about the many   thousands of pages that Stephen Kotkin has written 
about Stalin, obviously which we were not able to   exhaustively cover in that one 2-hour interview.
So anyways, if you want to see these additional   artifacts and writing that I produce as a result 
of this podcast and in preparation for episodes,   you should subscribe to my blog and newsletter.
You can do that at dwarkesh.com.  Otherwise I will also see you next week 
for a full episode with a real guest.